-*- markdown -*-

Services load / dump
====================

Remember the project dumps and loads run through all services a 
project defines and aggregate folder paths left behind each service's
dump process.

Where do files end up?
----------------------

By default the script could print the location of a tarball on the 
server - maybe ask "scp to home folder?"

The compiled dump could manifest.

    kraftwerk dump --manifest s3 --node myserver
    kraftwerk dump --node myserver

Exclude include services
------------------------

  kraftwerk dump --exclude files

CDN
===

Use s3fslite? It could be written like a files service except files 
get eventually synced to S3.

Templates
=========

Maybe scripts could be defined per-node or per-project. Similar to 
Django's contrib.admin fallthrough logic - from most specific to least
specific. Example loading pg_hba.conf:

    user_templates/hostname/pg_hba.conf
    user_templates/pg_hba.conf
    kraftwerk_templates/pg_hba.conf

Perhaps a --templates hook should be provided. Admins could then keep 
track of template folders. Kind of like puppet recipes but completely 
push based.

    kraftwerk setup-node cloud.server --templates ~/kraftwerk-recipes/master-slave-db

Deployment
==========

Maybe projects could specify a path to check for HTTP 1.0 200. It 
would serve as a last check at the gate before deployment.